FROM jupyter/pyspark-notebook

USER root

# Install curl
RUN apt-get update && \
    apt-get install -y curl && \
    rm -rf /var/lib/apt/lists/*

# Install PySpark to ensure it is available
RUN pip install pyspark==3.4.1

# Create the /opt/spark/jars/ directory before downloading the JAR files
RUN mkdir -p /opt/spark/jars/

# Download the Hadoop AWS JAR and the AWS SDK bundle
RUN curl -L -o /opt/spark/jars/hadoop-aws-3.3.1.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar && \
    curl -L -o /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

# Install and configure Jupyter logs
RUN mkdir -p /var/log/jupyter
RUN chown -R $NB_UID:$NB_GID /var/log/jupyter

# Use tee to write logs both to stdout (for Docker logs) and to a file
CMD ["sh", "-c", "jupyter notebook --notebook-dir=/home/jovyan/work --ip=0.0.0.0 --port=8888 --no-browser --allow-root 2>&1 | tee /var/log/jupyter/jupyter.log"]