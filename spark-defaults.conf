# S3 Configuration for MinIO
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=minioadmin
spark.hadoop.fs.s3a.secret.key=minioadmin
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200       # Maximum number of simultaneous connections to S3
spark.hadoop.fs.s3a.connection.timeout=5000      # Connection timeout in milliseconds
spark.hadoop.fs.s3a.attempts.maximum=20          # Maximum number of retry attempts
spark.hadoop.fs.s3a.retry.limit=10               # Retry limit for S3 operations
spark.hadoop.fs.s3a.threads.core=50              # Core thread pool size for S3A

# Spark Executor and Worker Settings
spark.executor.memory=2g                         # Amount of memory to use per executor process
spark.executor.cores=1                           # Number of cores to use on each executor
spark.executor.instances=2                       # Number of executors to launch for this application

# Dynamic Resource Allocation (Disabled)
# If you enable dynamic allocation, you must also enable and configure the external shuffle service
spark.dynamicAllocation.enabled=false
# spark.dynamicAllocation.minExecutors=1         # Minimum number of executors (if enabled)
# spark.dynamicAllocation.maxExecutors=4         # Maximum number of executors (if enabled)
# spark.dynamicAllocation.executorIdleTimeout=60s  # Time before idle executors are released (if enabled)
# spark.shuffle.service.enabled=true             # Must be true if dynamic allocation is enabled

# Optimize Shuffle Operations
spark.sql.shuffle.partitions=16                  # Number of partitions to use when shuffling data for joins or aggregations
spark.default.parallelism=16                     # Default number of tasks to use in RDD operations (should be >= total cores)

# JVM Garbage Collection Optimization
spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:G1ReservePercent=20
# -XX:+UseG1GC: Use the G1 garbage collector to reduce GC pause times
# -XX:InitiatingHeapOccupancyPercent=35: Start concurrent GC cycle when 35% of the heap is occupied
# -XX:G1ReservePercent=20: Reserve 20% of the heap as a buffer against overflow

# Spark Application Settings
spark.app.name=SparkClusterWithMinIO             # Name of your Spark application
spark.master=spark://spark-master:7077           # URL of the Spark Master

# Network Configuration
spark.network.timeout=600s                       # Default network timeout for all network interactions
spark.rpc.message.maxSize=512m                   # Maximum size of messages sent in RPC communication

# Logging Configuration
spark.eventLog.enabled=true                      # Enable event logging for Spark jobs
spark.eventLog.dir=/opt/spark/logs               # Directory where event logs are stored
# Ensure that this directory exists on all nodes and has the correct permissions
# Consider mapping this directory to a host volume in Docker Compose to persist logs

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer  # Use Kryo serialization for better performance
spark.kryoserializer.buffer.max=512m                         # Maximum buffer size for Kryo serialization

# Additional Settings (Optional)
spark.jars=/opt/spark/jars/hadoop-aws-3.3.1.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar
# Include if necessary; if jars are already in the classpath, this is not required

# Security Settings (Important for Production)
# Replace default MinIO credentials with secure ones in production environments
# spark.hadoop.fs.s3a.access.key=YOUR_SECURE_ACCESS_KEY
# spark.hadoop.fs.s3a.secret.key=YOUR_SECURE_SECRET_KEY
