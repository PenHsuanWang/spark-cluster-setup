{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48675406-5f48-4f3e-aa5c-fef13e4c6228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.11/site-packages/pip-23.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib in /opt/bitnami/python/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/bitnami/python/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/bitnami/python/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953d5dd6-fd61-499b-8d72-62e7ee2efd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, lag, lit\n",
    "from pyspark.sql.window import Window\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# start spark session and setup minio configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Integration\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter-notebook\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/hadoop-aws-3.3.1.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.threads.core\", \"20\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"5000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.limit\", \"10\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"104857600\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV files from MinIO for each company\n",
    "df_AAPL = spark.read.csv('s3a://your-bucket/AAPL.csv', header=True, inferSchema=True)\n",
    "df_AMZN = spark.read.csv('s3a://your-bucket/AMZN.csv', header=True, inferSchema=True)\n",
    "df_GOOG = spark.read.csv('s3a://your-bucket/GOOG.csv', header=True, inferSchema=True)\n",
    "df_MSFT = spark.read.csv('s3a://your-bucket/MSFT.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define a function to calculate moving average and daily return using Spark\n",
    "def calculate_metrics(df, company_name):\n",
    "    # Convert 'Date' column to date type\n",
    "    df = df.withColumn('Date', col('Date').cast('date'))\n",
    "    \n",
    "    # Calculate moving average (50-day)\n",
    "    windowSpec = Window.orderBy(\"Date\").rowsBetween(-49, 0)\n",
    "    df = df.withColumn('50_MA', avg(col('Close')).over(windowSpec))\n",
    "    \n",
    "    # Calculate daily returns using lag function\n",
    "    windowSpecLag = Window.orderBy(\"Date\")\n",
    "    df = df.withColumn('Prev_Close', lag(col('Close'), 1).over(windowSpecLag))\n",
    "    df = df.withColumn('Daily_Return', (col('Close') - col('Prev_Close')) / col('Prev_Close'))\n",
    "    \n",
    "    # Add a 'Company' column\n",
    "    df = df.withColumn('Company', lit(company_name))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to each company\n",
    "df_AAPL_metrics = calculate_metrics(df_AAPL, 'AAPL')\n",
    "df_AMZN_metrics = calculate_metrics(df_AMZN, 'AMZN')\n",
    "df_GOOG_metrics = calculate_metrics(df_GOOG, 'GOOG')\n",
    "df_MSFT_metrics = calculate_metrics(df_MSFT, 'MSFT')\n",
    "\n",
    "# Combine all the data into one DataFrame\n",
    "combined_df = df_AAPL_metrics.union(df_AMZN_metrics).union(df_GOOG_metrics).union(df_MSFT_metrics)\n",
    "\n",
    "# Show combined data\n",
    "combined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59279ee0-4097-4379-b3a2-f587563189ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-26 13:54:12.006404\n",
      "2024-10-26 13:54:12.102443\n",
      "declair_time_spend : 0:00:00.096039\n",
      "total process data count: 187\n",
      "total time spend after count : 0:00:00.172128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 214.9 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 114 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 277 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 34\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 74 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 108 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 108 (MapPartitionsRDD[277] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 16.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.3 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 108 (MapPartitionsRDD[277] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 74) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_108_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_113_piece0 on jupyter-notebook:34427 in memory (size: 17.0 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 172.18.0.7:33187 in memory (size: 17.0 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_112_piece0 on jupyter-notebook:34427 in memory (size: 7.2 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 172.18.0.7:33187 in memory (size: 7.2 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 74) in 18 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 108 (count at NativeMethodAccessorImpl.java:0) finished in 0.024 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 75 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 110 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 109)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 110 (MapPartitionsRDD[280] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 12.1 KiB, free 432.7 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 110 (MapPartitionsRDD[280] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 75) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 34 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 75) in 8 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 110 (count at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 110: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 75 finished: count at NativeMethodAccessorImpl.java:0, took 0.011179 s\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 214.9 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 117 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 284 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 35\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 76 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 111 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 111 (MapPartitionsRDD[284] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 16.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 111 (MapPartitionsRDD[284] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 76) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 76) in 14 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 111 (count at NativeMethodAccessorImpl.java:0) finished in 0.016 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 77 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 113 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 112)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[287] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 12.1 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[287] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 77) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 35 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 77) in 8 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 113 (count at NativeMethodAccessorImpl.java:0) finished in 0.011 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 77 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 77 finished: count at NativeMethodAccessorImpl.java:0, took 0.011014 s\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 214.9 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_117_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 120 from count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_115_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 291 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 36\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 78 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 114 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_114_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 114 (MapPartitionsRDD[291] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 16.6 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 114 (MapPartitionsRDD[291] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 78) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_119_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_118_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_116_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 78) in 16 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 114 (count at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 79 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 116 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 115)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[294] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 12.1 KiB, free 432.7 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 116 (MapPartitionsRDD[294] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 79) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 36 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 79) in 6 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 116 (count at NativeMethodAccessorImpl.java:0) finished in 0.009 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 79 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 79 finished: count at NativeMethodAccessorImpl.java:0, took 0.009429 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total process data count: 187\n",
      "total process data count: 187\n",
      "total process data count: 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 214.9 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 123 from count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 298 (count at <unknown>:0) as input to shuffle 37\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 80 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 117 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 117 (MapPartitionsRDD[298] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 16.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 117 (MapPartitionsRDD[298] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 80) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 80) in 18 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 117 (count at <unknown>:0) finished in 0.020 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 81 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 119 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 118)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[301] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 12.1 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[301] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 81) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 37 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_121_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 81) in 16 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 119 (count at <unknown>:0) finished in 0.018 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 81 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 81 finished: count at <unknown>:0, took 0.019379 s\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_120_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_124_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_122_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.2 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 214.9 KiB, free 432.5 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 126 from count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 305 (count at <unknown>:0) as input to shuffle 38\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 82 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 120 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 120 (MapPartitionsRDD[305] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 16.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 120 (MapPartitionsRDD[305] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 82) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 82) in 15 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 120 (count at <unknown>:0) finished in 0.018 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 83 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 122 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 121)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 122 (MapPartitionsRDD[308] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 12.1 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 122 (MapPartitionsRDD[308] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 83) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 38 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 83) in 7 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 122 (count at <unknown>:0) finished in 0.009 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 83 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 122: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 83 finished: count at <unknown>:0, took 0.009468 s\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 214.9 KiB, free 432.2 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 129 from count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 312 (count at <unknown>:0) as input to shuffle 39\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 84 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 123 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 123 (MapPartitionsRDD[312] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 16.6 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 123 (MapPartitionsRDD[312] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 84) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 84) in 14 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 123 (count at <unknown>:0) finished in 0.015 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 85 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 125 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 124)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[315] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 12.1 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 125 (MapPartitionsRDD[315] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 85) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_127_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_123_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total process data count: 187\n",
      "total process data count: 187\n",
      "total process data count: 187\n",
      "total time spend after count : 0:00:00.611757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 39 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_126_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 85) in 15 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 125 (count at <unknown>:0) finished in 0.022 s\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_128_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 85 finished: count at <unknown>:0, took 0.022979 s\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_130_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_125_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.2 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.6 MiB)\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 214.9 KiB, free 432.5 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 132 from count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 319 (count at <unknown>:0) as input to shuffle 40\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 86 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 126 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[319] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 16.6 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on jupyter-notebook:34427 (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[319] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 86) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 172.18.0.7:33187 (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 86) in 14 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 126 (count at <unknown>:0) finished in 0.016 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: count at <unknown>:0\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 87 (count at <unknown>:0) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 128 (count at <unknown>:0)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 127)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[322] at count at <unknown>:0), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 12.1 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.4 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on jupyter-notebook:34427 (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[322] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 87) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 172.18.0.7:33187 (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 40 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 87) in 6 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 128 (count at <unknown>:0) finished in 0.007 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 87 finished: count at <unknown>:0, took 0.007963 s\n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/10/26 13:54:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/10/26 13:54:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/10/26 13:54:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/10/26 13:54:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 214.9 KiB, free 432.2 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on jupyter-notebook:34427 (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 135 from toPandas at /tmp/ipykernel_31/3852980714.py:35\n",
      "24/10/26 13:54:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Registering RDD 325 (toPandas at /tmp/ipykernel_31/3852980714.py:35) as input to shuffle 41\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got map stage job 88 (toPandas at /tmp/ipykernel_31/3852980714.py:35) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ShuffleMapStage 129 (toPandas at /tmp/ipykernel_31/3852980714.py:35)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ShuffleMapStage 129 (MapPartitionsRDD[325] at toPandas at /tmp/ipykernel_31/3852980714.py:35), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 13.4 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 432.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on jupyter-notebook:34427 (size: 7.2 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_132_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 129 (MapPartitionsRDD[325] at toPandas at /tmp/ipykernel_31/3852980714.py:35) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 88) (172.18.0.7, executor 0, partition 0, PROCESS_LOCAL, 7899 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_131_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 172.18.0.7:33187 (size: 7.2 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_134_piece0 on jupyter-notebook:34427 in memory (size: 5.8 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 172.18.0.7:33187 in memory (size: 5.8 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_133_piece0 on jupyter-notebook:34427 in memory (size: 8.5 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 172.18.0.7:33187 in memory (size: 8.5 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 172.18.0.7:33187 (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_129_piece0 on jupyter-notebook:34427 in memory (size: 35.6 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 172.18.0.7:33187 in memory (size: 35.6 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 88) in 17 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ShuffleMapStage 129 (toPandas at /tmp/ipykernel_31/3852980714.py:35) finished in 0.023 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/10/26 13:54:12 INFO DAGScheduler: running: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: waiting: Set()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: failed: Set()\n",
      "24/10/26 13:54:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/10/26 13:54:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/10/26 13:54:12 INFO SparkContext: Starting job: toPandas at /tmp/ipykernel_31/3852980714.py:35\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Got job 89 (toPandas at /tmp/ipykernel_31/3852980714.py:35) with 1 output partitions\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Final stage: ResultStage 131 (toPandas at /tmp/ipykernel_31/3852980714.py:35)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 130)\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[329] at toPandas at /tmp/ipykernel_31/3852980714.py:35), which has no missing parents\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 36.3 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 432.6 MiB)\n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on jupyter-notebook:34427 (size: 17.0 KiB, free: 434.1 MiB)\n",
      "24/10/26 13:54:12 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1535\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[329] at toPandas at /tmp/ipykernel_31/3852980714.py:35) (first 15 tasks are for partitions Vector(0))\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks resource profile 0\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 89) (172.18.0.7, executor 0, partition 0, NODE_LOCAL, 7367 bytes) \n",
      "24/10/26 13:54:12 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 172.18.0.7:33187 (size: 17.0 KiB, free: 1048.5 MiB)\n",
      "24/10/26 13:54:12 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 41 to 172.18.0.7:37740\n",
      "24/10/26 13:54:12 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 89) in 14 ms on 172.18.0.7 (executor 0) (1/1)\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool \n",
      "24/10/26 13:54:12 INFO DAGScheduler: ResultStage 131 (toPandas at /tmp/ipykernel_31/3852980714.py:35) finished in 0.017 s\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/10/26 13:54:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished\n",
      "24/10/26 13:54:12 INFO DAGScheduler: Job 89 finished: toPandas at /tmp/ipykernel_31/3852980714.py:35, took 0.017232 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "#  Daily_Return \n",
    "df_AAPL_returns = df_AAPL_metrics.select('Date', col('Daily_Return').alias('AAPL_Daily_Return'))\n",
    "df_AMZN_returns = df_AMZN_metrics.select('Date', col('Daily_Return').alias('AMZN_Daily_Return'))\n",
    "df_GOOG_returns = df_GOOG_metrics.select('Date', col('Daily_Return').alias('GOOG_Daily_Return'))\n",
    "df_MSFT_returns = df_MSFT_metrics.select('Date', col('Daily_Return').alias('MSFT_Daily_Return'))\n",
    "\n",
    "# Join  DataFrame\n",
    "df_joined = df_AAPL_returns.join(df_AMZN_returns, 'Date', 'inner') \\\n",
    "                           .join(df_GOOG_returns, 'Date', 'inner') \\\n",
    "                           .join(df_MSFT_returns, 'Date', 'inner')\n",
    "\n",
    "# \n",
    "correlation_AAPL_AMZN = df_joined.stat.corr('AAPL_Daily_Return', 'AMZN_Daily_Return')\n",
    "correlation_AAPL_GOOG = df_joined.stat.corr('AAPL_Daily_Return', 'GOOG_Daily_Return')\n",
    "correlation_AAPL_MSFT = df_joined.stat.corr('AAPL_Daily_Return', 'MSFT_Daily_Return')\n",
    "\n",
    "correlation_AMZN_GOOG = df_joined.stat.corr('AMZN_Daily_Return', 'GOOG_Daily_Return')\n",
    "correlation_AMZN_MSFT = df_joined.stat.corr('AMZN_Daily_Return', 'MSFT_Daily_Return')\n",
    "\n",
    "correlation_GOOG_MSFT = df_joined.stat.corr('GOOG_Daily_Return', 'MSFT_Daily_Return')\n",
    "\n",
    "#  Pandas DataFrame \n",
    "correlation_data = {\n",
    "    'AAPL': [1, correlation_AAPL_AMZN, correlation_AAPL_GOOG, correlation_AAPL_MSFT],\n",
    "    'AMZN': [correlation_AAPL_AMZN, 1, correlation_AMZN_GOOG, correlation_AMZN_MSFT],\n",
    "    'GOOG': [correlation_AAPL_GOOG, correlation_AMZN_GOOG, 1, correlation_GOOG_MSFT],\n",
    "    'MSFT': [correlation_AAPL_MSFT, correlation_AMZN_MSFT, correlation_GOOG_MSFT, 1]\n",
    "}\n",
    "\n",
    "#  Pandas DataFrame\n",
    "correlation_df = pd.DataFrame(correlation_data, index=['AAPL', 'AMZN', 'GOOG', 'MSFT'])\n",
    "\n",
    "#  Seaborn \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_df, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Daily Returns')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
